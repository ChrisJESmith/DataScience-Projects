{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Spam Messages Using NLP & Naive Bayes\n",
    "\n",
    "In this project, we will build a very simple SPAM detector for SMS messages.\n",
    "\n",
    "The SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. \n",
    "\n",
    "It contains one set of SMS messages in English of 5,574 messages, tagged according being ham (legitimate) or spam. The distribution is a total of 4,827 SMS legitimate messages (86.6%) and a total of 747 (13.4%) spam messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction To The Dataset\n",
    "\n",
    "We will import the data and define a separator (in this case, a tab) and rename the columns accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_table(r'C:\\projectdatasets\\SMSSpamCollection',  \n",
    "                   sep='\\t', \n",
    "                   header=None,\n",
    "                   names=['label', 'message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the data has the format [label] [tab] [message]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "5  spam  FreeMsg Hey there darling it's been 3 week's n...\n",
       "6   ham  Even my brother is not like to speak with me. ...\n",
       "7   ham  As per your request 'Melle Melle (Oru Minnamin...\n",
       "8  spam  WINNER!! As a valued network customer you have...\n",
       "9  spam  Had your mobile 11 months or more? U R entitle..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the first ten rows\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there are 5572 rows and two columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing\n",
    "\n",
    "We need to perform some data cleansing on the data before we apply our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert labels from strings to binary values for our classifier\n",
    "df['label'] = df.label.map({'ham': 0, 'spam': 1})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    1\n",
       "3    0\n",
       "4    0\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the label field now has 0s and 1s to indicate ham and spam\n",
    "df['label'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all characters in the message to lower case\n",
    "df['message'] = df.message.map(lambda x: x.lower())\n",
    "\n",
    "# remove any punctuation:\n",
    "df['message'] = df.message.str.replace('[^\\w\\s]', '')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    go until jurong point crazy available only in ...\n",
       "1                              ok lar joking wif u oni\n",
       "2    free entry in 2 a wkly comp to win fa cup fina...\n",
       "3          u dun say so early hor u c already then say\n",
       "4    nah i dont think he goes to usf he lives aroun...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the message field has been cleansed\n",
    "df['message'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Natural Language Toolkit (NLTK), is a suite of libraries and programs for natural language processing for Python\n",
    "We will import the 'nltk' package (once we download it) to apply tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk  \n",
    "# nltk.download()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the messages into into single words using nltk\n",
    "df['message'] = df['message'].apply(nltk.word_tokenize)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [go, until, jurong, point, crazy, available, o...\n",
       "1                       [ok, lar, joking, wif, u, oni]\n",
       "2    [free, entry, in, 2, a, wkly, comp, to, win, f...\n",
       "3    [u, dun, say, so, early, hor, u, c, already, t...\n",
       "4    [nah, i, dont, think, he, goes, to, usf, he, l...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the message field has been tokenised\n",
    "df['message'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform word stemming (normalize text for all variations of words that carry the same meaning, regardless of the tense) \n",
    "# a popular stemming algorithm is the Porter Stemmer\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "df['message'] = df['message'].apply(lambda x: [stemmer.stem(y) for y in x])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [go, until, jurong, point, crazi, avail, onli,...\n",
       "1                         [ok, lar, joke, wif, u, oni]\n",
       "2    [free, entri, in, 2, a, wkli, comp, to, win, f...\n",
       "3    [u, dun, say, so, earli, hor, u, c, alreadi, t...\n",
       "4    [nah, i, dont, think, he, goe, to, usf, he, li...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the message field has been word stemmed (can see 'crazy', 'entry' has been replaced with 'crazi', 'entri' etc.)\n",
    "df['message'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will transform the data into occurrences, which will be the features that will feed into the model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# convert the list of words into space-separated strings\n",
    "df['message'] = df['message'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    go until jurong point crazi avail onli in bugi...\n",
       "1                                ok lar joke wif u oni\n",
       "2    free entri in 2 a wkli comp to win fa cup fina...\n",
       "3          u dun say so earli hor u c alreadi then say\n",
       "4    nah i dont think he goe to usf he live around ...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the message field has been converted to space separated strings (like it was at the beginning)\n",
    "df['message'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5572x8169 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 72500 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer()  \n",
    "counts = count_vect.fit_transform(df['message'])\n",
    "\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could leave it as the simple word-count per message, but it is better to use Term Frequency Inverse Document Frequency \n",
    "# this is more known as tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer().fit(counts)\n",
    "counts = transformer.transform(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)\n"
     ]
    }
   ],
   "source": [
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7925)\t0.22378642176936625\n",
      "  (0, 7715)\t0.18293604147358436\n",
      "  (0, 7497)\t0.232012730496152\n",
      "  (0, 7130)\t0.15808501470085967\n",
      "  (0, 5635)\t0.22485506312666312\n",
      "  (0, 5292)\t0.1588008730270491\n",
      "  (0, 4273)\t0.2781965206152583\n",
      "  (0, 4128)\t0.32930301835453774\n",
      "  (0, 3872)\t0.10860920003212803\n",
      "  (0, 3425)\t0.18328548053939198\n",
      "  (0, 3388)\t0.15280952404957904\n",
      "  (0, 3336)\t0.132266862568599\n",
      "  (0, 2248)\t0.255022519528138\n",
      "  (0, 2029)\t0.2781965206152583\n",
      "  (0, 1750)\t0.2781965206152583\n",
      "  (0, 1748)\t0.31435532599420324\n",
      "  (0, 1340)\t0.2504083119963028\n",
      "  (0, 1146)\t0.32930301835453774\n",
      "  (1, 7835)\t0.44483654514496557\n",
      "  (1, 5289)\t0.5633498837724461\n",
      "  (1, 5257)\t0.2825014776211812\n",
      "  (1, 4308)\t0.42081977871680865\n",
      "  (1, 4094)\t0.4773478663822099\n",
      "  (2, 7883)\t0.18653623125647448\n",
      "  (2, 7848)\t0.14242759355834578\n",
      "  :\t:\n",
      "  (5570, 6587)\t0.19054252105358732\n",
      "  (5570, 5048)\t0.21643786562194572\n",
      "  (5570, 4396)\t0.16284308112975754\n",
      "  (5570, 3987)\t0.11780359009346424\n",
      "  (5570, 3940)\t0.27149395792904457\n",
      "  (5570, 3872)\t0.1156240697440695\n",
      "  (5570, 3823)\t0.23824037771001683\n",
      "  (5570, 3559)\t0.17834782034673616\n",
      "  (5570, 3477)\t0.2151717357048722\n",
      "  (5570, 3255)\t0.28366649166348035\n",
      "  (5570, 3148)\t0.16521213094749893\n",
      "  (5570, 3105)\t0.12517861385377763\n",
      "  (5570, 2760)\t0.25144577693862574\n",
      "  (5570, 2492)\t0.19259063566649914\n",
      "  (5570, 1777)\t0.21160140056576388\n",
      "  (5570, 1773)\t0.14185384464898346\n",
      "  (5570, 1563)\t0.301405381695999\n",
      "  (5570, 1463)\t0.14322419016375695\n",
      "  (5570, 1160)\t0.11592101913259747\n",
      "  (5570, 983)\t0.3074552754076518\n",
      "  (5571, 7366)\t0.47146981576404473\n",
      "  (5571, 7236)\t0.1625649279501769\n",
      "  (5571, 6114)\t0.6100619543472223\n",
      "  (5571, 4970)\t0.42607428138503617\n",
      "  (5571, 3987)\t0.4444929421534656\n"
     ]
    }
   ],
   "source": [
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training The Model\n",
    "\n",
    "Now we have performed feature extraction from our data, we will build our model. \n",
    "\n",
    "We will start by splitting our data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(counts, df['label'], test_size=0.1, random_state=69)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 989)\t0.28052144773473153\n",
      "  (0, 1072)\t0.3432827956011483\n",
      "  (0, 1184)\t0.412788938318317\n",
      "  (0, 1304)\t0.1705583078355282\n",
      "  (0, 1773)\t0.16702897405622036\n",
      "  (0, 3732)\t0.26222639184071767\n",
      "  (0, 3850)\t0.16406777394283956\n",
      "  (0, 3976)\t0.1388949438216507\n",
      "  (0, 5088)\t0.17825595732136162\n",
      "  (0, 5540)\t0.21779783920283172\n",
      "  (0, 5745)\t0.3166941755901548\n",
      "  (0, 5951)\t0.3041276257048051\n",
      "  (0, 6508)\t0.3940516607477597\n",
      "  (0, 7797)\t0.1839341921120709\n",
      "  (1, 211)\t0.28267413293218063\n",
      "  (1, 359)\t0.238804249783713\n",
      "  (1, 582)\t0.2536777174054881\n",
      "  (1, 1228)\t0.2698430146557444\n",
      "  (1, 1356)\t0.17557364291658975\n",
      "  (1, 1805)\t0.10254684297717932\n",
      "  (1, 1905)\t0.22109496673870935\n",
      "  (1, 2166)\t0.21686930820947917\n",
      "  (1, 2746)\t0.20538500729287698\n",
      "  (1, 3193)\t0.12744459627924393\n",
      "  (1, 3307)\t0.20031089495440416\n",
      "  :\t:\n",
      "  (5011, 8130)\t0.22336964227394898\n",
      "  (5012, 1132)\t0.16877711316090904\n",
      "  (5012, 1160)\t0.23405780291790354\n",
      "  (5012, 1631)\t0.24796824405972512\n",
      "  (5012, 1852)\t0.19136883768188218\n",
      "  (5012, 2368)\t0.16661539279372278\n",
      "  (5012, 2694)\t0.24406787983426736\n",
      "  (5012, 2987)\t0.20536082093089142\n",
      "  (5012, 3028)\t0.22968894283882926\n",
      "  (5012, 3691)\t0.1800523291544093\n",
      "  (5012, 3869)\t0.31761758440576265\n",
      "  (5012, 4129)\t0.14816875063172014\n",
      "  (5012, 5054)\t0.2211936566420455\n",
      "  (5012, 5155)\t0.13828468426776297\n",
      "  (5012, 5325)\t0.1444925794995165\n",
      "  (5012, 6017)\t0.24051729268077796\n",
      "  (5012, 6313)\t0.3539225919965262\n",
      "  (5012, 7669)\t0.3429157293054803\n",
      "  (5012, 7778)\t0.22407650683100214\n",
      "  (5012, 7803)\t0.1606079871209852\n",
      "  (5012, 8130)\t0.12439347453441892\n",
      "  (5013, 1304)\t0.4087126133036288\n",
      "  (5013, 3691)\t0.5032266943277521\n",
      "  (5013, 3850)\t0.3931592046032174\n",
      "  (5013, 5939)\t0.6520297030643981\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 895)\t0.28999797162758306\n",
      "  (0, 1124)\t0.17503953462676497\n",
      "  (0, 1260)\t0.1806775784485796\n",
      "  (0, 1605)\t0.28999797162758306\n",
      "  (0, 1773)\t0.11734341495843983\n",
      "  (0, 2055)\t0.26025028442328957\n",
      "  (0, 2109)\t0.12945014502730604\n",
      "  (0, 3028)\t0.1882030959165165\n",
      "  (0, 3930)\t0.28999797162758306\n",
      "  (0, 3976)\t0.2927349662641272\n",
      "  (0, 3985)\t0.24116759238885288\n",
      "  (0, 5227)\t0.10770668790321371\n",
      "  (0, 5380)\t0.28999797162758306\n",
      "  (0, 5491)\t0.27683441033783157\n",
      "  (0, 6049)\t0.28999797162758306\n",
      "  (0, 6579)\t0.267494714968356\n",
      "  (0, 6780)\t0.14918636698714263\n",
      "  (0, 7109)\t0.08721601188417048\n",
      "  (0, 7846)\t0.12158774440143195\n",
      "  (0, 7919)\t0.15447508303473523\n",
      "  (0, 8130)\t0.10192583382482392\n",
      "  (1, 1160)\t0.1073266226344332\n",
      "  (1, 1417)\t0.2846605090910865\n",
      "  (1, 1597)\t0.26992717420861423\n",
      "  (1, 3336)\t0.13037008878027842\n",
      "  :\t:\n",
      "  (554, 7681)\t0.139088862729437\n",
      "  (554, 7871)\t0.25318299772379493\n",
      "  (554, 8113)\t0.06505727058380065\n",
      "  (554, 8130)\t0.2669593308715244\n",
      "  (555, 1304)\t0.4081000495497748\n",
      "  (555, 1805)\t0.3583092712410307\n",
      "  (555, 3850)\t0.39256995173850323\n",
      "  (555, 5595)\t0.5235979140724424\n",
      "  (555, 7919)\t0.5261205879560402\n",
      "  (556, 1242)\t0.20253078707172145\n",
      "  (556, 1274)\t0.5251556172117957\n",
      "  (556, 3294)\t0.2055930040425291\n",
      "  (556, 3732)\t0.32516316352829827\n",
      "  (556, 4315)\t0.3004779768558425\n",
      "  (556, 4532)\t0.3859543913296104\n",
      "  (556, 4753)\t0.34784920290696647\n",
      "  (556, 4916)\t0.2832810602438849\n",
      "  (556, 6587)\t0.2782066780848101\n",
      "  (556, 8113)\t0.13152681180336454\n",
      "  (557, 4676)\t0.27296197250004434\n",
      "  (557, 6369)\t0.43189897372393304\n",
      "  (557, 6989)\t0.4865679994383735\n",
      "  (557, 7236)\t0.3885735363807028\n",
      "  (557, 7703)\t0.3686591759960736\n",
      "  (557, 7815)\t0.4640125416043495\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096    0\n",
       "866     1\n",
       "1732    0\n",
       "1260    0\n",
       "5415    0\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can run the 'head' command to check the data, as this is a dataframe\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3444    0\n",
       "378     0\n",
       "3330    0\n",
       "4606    0\n",
       "2050    0\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can run the 'head' command to check the data, as this is a dataframe\n",
    "y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to initialize the Naive Bayes Classifier and fit the training data\n",
    "# for text classification problems, the Multinomial Naive Bayes Classifier is well-suited\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB().fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating The Model\n",
    "\n",
    "We have now put together our classifier, so we can evaluate its performance using the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# run predictions using the test dataset\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9480286738351255\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(predicted == y_test))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive Bayes Classifier has a 94.2% accuracy with this test set. Note that 'accuracy' might not be a good assessment, since the dataset is imbalanced when it comes to the labels (86.6% legitimate vs 13.4% spam)\n",
    "\n",
    "It could be that our classifier is over-fitting the legitimate class, while ignoring the spam class. To solve this uncertainty, we will look at a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[482   0]\n",
      " [ 29  47]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, predicted))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the amount of errors is relatively balanced between legitimate and spam, with 0 legitimate messages classified as spam and 29 spam messages classified as legitimate. Overall, these are good results for our simple classifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
